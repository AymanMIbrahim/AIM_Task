{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from flask import Flask,request,jsonify\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6498e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"LSTM-TEXT-Classifictaion_V2-03-0.55.hdf5\")\n",
    "with open(\"X_Data.pkl\", \"rb\") as DS:\n",
    "    X = pickle.load(DS)\n",
    "with open(\"ML_Model.pkl\", \"rb\") as DS:\n",
    "    modelML = pickle.load(DS)\n",
    "with open(\"Word2ID.pkl\", \"rb\") as DS:\n",
    "    word2ID = pickle.load(DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fcf883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb62931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess(text,word2ID):\n",
    "    Final_Preprocced_Data = []\n",
    "    Sent = text\n",
    "    D = remove_emojis(Sent)\n",
    "    tempList = []\n",
    "    SplitedData = re.split(r'[a-z 1 2 3 4 5 6 7 8 9 0 ^ ( ) , ، % $ : ; \"  > < ! ? ؟  ; _ -|, |\\*|\\n . @ ? ! # / \\xa0]'\n",
    "                           ,D) \n",
    "    for words in SplitedData:\n",
    "        if (words != \"\"):\n",
    "            tempList.append(words)\n",
    "\n",
    "    WholeArabicSent = ' '.join(tempList)\n",
    "    Final_Preprocced_Data.append(WholeArabicSent)\n",
    "    \n",
    "    X_Vector = []\n",
    "    for data in Final_Preprocced_Data:\n",
    "        Vec = []\n",
    "        words = nltk.word_tokenize(data)\n",
    "        for word in words:\n",
    "            if word in word2ID.keys():\n",
    "                Id = word2ID[word]\n",
    "                Vec.append(Id)\n",
    "        \n",
    "        X_Vector.append(Vec)\n",
    "    \n",
    "    X_Vector = tf.keras.preprocessing.sequence.pad_sequences(X_Vector, maxlen=90)\n",
    "    X_Vector = np.array(X_Vector,dtype=np.int32)\n",
    "    \n",
    "    return X_Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessML(text):\n",
    "    X.append(text)\n",
    "    tfidfconverter = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.7, stop_words=stopwords.words('arabic'))\n",
    "    X_Vec = tfidfconverter.fit_transform(X).toarray()\n",
    "    \n",
    "    return X_Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b9fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/test',methods=[\"POST\"])\n",
    "def Test():\n",
    "    if request.method == \"POST\":\n",
    "        ListOfOutput = ['AE','BH','DZ','EG','IQ','JO','KW','LB','LY','MA','OM','PL','QA','SA','SD','SY','TN','YE']\n",
    "        req_json = request.json\n",
    "        test_Sent = req_json[\"Sent\"]\n",
    "        Input = PreProcess(test_Sent,word2ID)\n",
    "        InputML = PreProcessML(test_Sent)\n",
    "        Res = model.predict(Input)\n",
    "        ResML = modelML.predict([InputML[len(X)-1]])\n",
    "        Index = Res.argmax()\n",
    "        IndexML = ResML.argmax()\n",
    "        Output = ListOfOutput[Index]\n",
    "        OutputML = ListOfOutput[IndexML]\n",
    "        \n",
    "        return jsonify({\"response_DL\":Output,\"response_ML\":Output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f129d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=False,port=9090)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
